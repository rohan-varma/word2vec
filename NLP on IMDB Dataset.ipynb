{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### NLP on IMDB Dataset\n",
    "\n",
    "This notebook is to investigate the basics of natural language processing, specifically to the application of classifying sentiment. I used a large dataset of IMDB movie reviews, and tried to classify each review as positive or negative. My initial model used a vanilla bag of words method, treating each text as an unordered multiset of the words that it contains. I then attempted to use the word2vec neural network model in order to learn more sophisticated word representations, to gain better classification accuracy. \n",
    "\n",
    "### The Problem\n",
    "\n",
    "We have several example movie reviews, that were written by people after watching a movie. We want to learn a function $f: X \\rightarrow{} y$ that takes in these reviews and outputs a binary value indicating whether the review indicates a positive or negative sentiment about the movie. \n",
    "\n",
    "We assume that there is some (unknown) true data generating distribution $D$ that defines $p(x, y)$, or the probability of observing a certain sentiment given a particular review. This distribution defines such probabilities for all pairs $(x, y) \\in X \\times Y$, but we only have access to a very small subset of the data (ie, the training data). Our function $f$ must be able to generalize well, and we define this as having the minimum loss on unseen data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, we read in our labelled training data using `pandas`. The reviews will be held in the data structure train[\"review\"]. Let's take a look at a single review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "import pandas as pd\n",
    "train = pd.read_csv('data/labeledTrainData.tsv', header = 0, delimiter = '\\t', quoting = 3)\n",
    "print(train[\"review\"][0])\n",
    "print(train[\"sentiment\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, it is required to clean each review so that we eliminate punctuation and extract only the words. Moreover, since some words occur so many times in the English language (such as \"the\", \"a\", \"it\", etc), we don't want to consider those words in our model. This is because having them or not having them in our training data won't really make a difference - they are so common and don't indicate any sort of sentiment, so we would lose nothing by throwing them out. In fact, we'd make the gain of not having to represent these words in our model, which saves us a bit of computation time as well as space. \n",
    "\n",
    "The BeautifulSoup library was used to extract only the letters. Stanford's NLTK was used to find \"stopwords\", or words that occur very often in the English language. The following cell is heavily based off of this Kaggle tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# cleaning data\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "def clean(review):\n",
    "    # remove html\n",
    "    text = BeautifulSoup(review, \"html5lib\").get_text()\n",
    "    # regexp matching to extract letters only\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # remove common words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return (\" \".join(meaningful_words))\n",
    "\n",
    "cleaned_reviews = [clean(train[\"review\"][i]) for i in range(len(train[\"review\"]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, we can look at the words in a clean review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['classic', 'war', 'worlds', 'timothy', 'hines', 'entertaining', 'film', 'obviously', 'goes', 'great', 'effort', 'lengths', 'faithfully', 'recreate', 'h', 'g', 'wells', 'classic', 'book', 'mr', 'hines', 'succeeds', 'watched', 'film', 'appreciated', 'fact', 'standard', 'predictable', 'hollywood', 'fare', 'comes', 'every', 'year', 'e', 'g', 'spielberg', 'version', 'tom', 'cruise', 'slightest', 'resemblance', 'book', 'obviously', 'everyone', 'looks', 'different', 'things', 'movie', 'envision', 'amateur', 'critics', 'look', 'criticize', 'everything', 'others', 'rate', 'movie', 'important', 'bases', 'like', 'entertained', 'people', 'never', 'agree', 'critics', 'enjoyed', 'effort', 'mr', 'hines', 'put', 'faithful', 'h', 'g', 'wells', 'classic', 'novel', 'found', 'entertaining', 'made', 'easy', 'overlook', 'critics', 'perceive', 'shortcomings']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_reviews[1].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we begin to define the core functions that allow us to take English words and represent them as numerical training data, which is what our machine learning algorithms actuall look at. This requires a few steps:\n",
    "\n",
    "- Creating a vocabulary, which is just a set of all the words in all the reviews. \n",
    "\n",
    "- Obtaining, for each review, an occurence dictionary. This function will take a single review and return a dictionary that enumerates how often each word occurred. \n",
    "\n",
    "- Using the above two functions, we can define another function that actually creates the feature vectors for our bag of words model. If there are n words in the vocabulary, then for each review, the feature vector f corresponding to that review will have values f[i] that correspond to the number of times that particular word occurred in the review (and 0 if it was not present in the review). \n",
    "\n",
    "You may notice that this \"bag of words\" model already has a few weaknesses. Most significantly, it does not take into account the ordering of words or any sense of context. The English langauge is full of phrases and idioms that are composed of words that when put together, mean something entirely different than the two words separately. \n",
    "\n",
    "However, despite its disadvantages, the bag of words model has actually seen some significant success in practice, most notably for spam filtering. This makes sense - even if spam emails do have phrases where the words greatly depend on the context around them, we can probably get really good spam classification just by detecting the presence and relative frequency of certain words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "# creates a vocabulary - set of all words in all reviews\n",
    "def create_vocab(cleaned_reviews):\n",
    "    \"\"\"\n",
    "    Takes in a bunch of reviews and creates a vocabulary. \n",
    "    \"\"\"\n",
    "    li = []\n",
    "    for review in cleaned_reviews:\n",
    "        a = review.split()\n",
    "        for item in a:\n",
    "            li.append(item)\n",
    "    return list(set(li))\n",
    "\n",
    "def get_word_occ_dict(review):\n",
    "    d = defaultdict(int)\n",
    "    words = review.split()\n",
    "    for w in words:\n",
    "        d[w]+=1\n",
    "    return d\n",
    "\n",
    "# takes in a vocab and a review and returns a feature vector for the review\n",
    "# the feature vector f has d dimensions where d = len (vocab)\n",
    "# for the i in [1..d]th word, f[i] = n where n is the number of times the word occured in the review\n",
    "# the feature vectors are sparse, since most words in the vocab may not occur in a specific review\n",
    "def create_feature_vector(review, vocab):\n",
    "    word_dict = get_word_occ_dict(review) \n",
    "    feature_vector = [word_dict[v] if v in word_dict else 0 for v in vocab]\n",
    "    return np.array(feature_vector)\n",
    "\n",
    "def create_feature_vectors(cleaned_reviews, vocab):\n",
    "    feature_vectors = [create_feature_vector(review, vocab) for review in cleaned_reviews]\n",
    "    return np.array(feature_vectors)\n",
    "\n",
    "vocab = create_vocab(cleaned_reviews)\n",
    "X = create_feature_vectors(cleaned_reviews, vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, let's get our labels for our training data, and look at the shape of both our training and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = train['sentiment']\n",
    "X.shape\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we get on to the actual learning portion of our investigation. To do this, I first imported a lot of functions I'll be using, mostly from the Sci-kit learn library. I also imported a personal machine learning utilities library that I wrote up to help me with cross-validation and hyperparameter tuning needs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# separate data into training and testing\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size = 0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, I fit a single SVM to the data before considering hyperparameter settings and other complex models in order to see what kind of accuracy I'm getting. In order to save a lot of computation time, I decided not to engineer any additional features or do any sort of feature expansion. SVMs can be kernelized, which means that a kernel function can replace the feature transformations anyways, so we can use that if we want to consider higher-dimensional spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/svm/base.py:920: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 1.0\n",
      "training accuracy: 0.8594666666666667\n"
     ]
    }
   ],
   "source": [
    "# fit SVM to the data\n",
    "clf = LinearSVC(verbose = 10)\n",
    "clf.fit(X_train, y_train)\n",
    "y_train_pred, y_test_pred = clf.predict(X_train), clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_acc, train_acc = accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)\n",
    "print(\"test accuracy: {}\".format(test_acc))\n",
    "print(\"training accuracy: {}\".format(train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, I tried out several different linear SVMs, primarily by changing the hyperparameter `C`. To avoid rehashing the explanation for exactly what this hyperparameter does, refer to my Quora answer for an explanation. \n",
    "\n",
    "I also used a utility function that I wrote in my personal utils library called `get_best_hyperparams_cv`. \n",
    "This function takes in a bunch of classifiers with different hyperparameter settings, and returns the classifier and setting that performs the best (where best is defined by loweset cross-validation error). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with params: 0.01\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]found params with test error: 0.12194285714285713\n",
      "training with params: 0.1\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
     ]
    }
   ],
   "source": [
    "# try several different classifiers by changing the value for C, which indicates how much slack variables are penalized.\n",
    "clfs_and_params = [(LinearSVC(C = c, verbose = 10), c) for c in [0.01, 0.1, 1.0, 5.0, 10, 100]]\n",
    "clf, best_params, best_test_err, best_train_err = get_best_hyperparams_cv(X_train, y_train, k = 10, \n",
    "                                                                          classifiers = clfs_and_params, \n",
    "                                                                          verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/unlabeledTrainData.tsv', header = 0, delimiter = '\\t', quoting = 3)\n",
    "cleaned_reviews = [clean(test_data[\"review\"][i]) for i in range(len(test_data[\"review\"]))]\n",
    "X = create_feature_vectors(cleaned_reviews, vocab)\n",
    "final_test_preds = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# cleaned reviews are a bunch of reviews where we will get our training examples from.\n",
    "# Let's look at one cleaned review: \n",
    "print(cleaned_reviews[0])\n",
    "window_size = 1\n",
    "vocab = create_vocab(cleaned_reviews)\n",
    "\n",
    "def word_one_hot(word, vocab):\n",
    "    idx = vocab.index(word)\n",
    "    if idx < 0:\n",
    "        return -1\n",
    "    vec = np.zeros((len(vocab)))\n",
    "    vec[idx] = 1\n",
    "    return vec\n",
    "\n",
    "def create_vectorized_word_pairs(review, vocab, window_size):\n",
    "    words = review.split()\n",
    "    data = []\n",
    "    for i in range(len(words)):\n",
    "        left = [words[i-j] for j in range(1, window_size + 1) if i-j >= 0]\n",
    "        right = [words[i+j] for j in range(1, window_size + 1) if i+j < len(words)]\n",
    "        neighbors = left + right\n",
    "        pairs = [(word_one_hot(words[i], vocab), word_one_hot(n,vocab)) for n in neighbors]\n",
    "        data.append(pairs)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_word_pairs_all_reviews(cleaned_reviews, vocab, window_size):\n",
    "    data = []\n",
    "    for review in cleaned_reviews:\n",
    "        li = create_vectorized_word_pairs(review, vocab, window_size)\n",
    "        data = data + li\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([ 0.,  0.,  0., ...,  0.,  0.,  0.]), array([ 0.,  0.,  0., ...,  0.,  0.,  0.]))]\n",
      "stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter\n"
     ]
    }
   ],
   "source": [
    "example_pairs = create_vectorized_word_pairs(cleaned_reviews[0], vocab, 1)\n",
    "print(example_pairs[0])\n",
    "print(cleaned_reviews[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-c3c4a8997a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# see how many of the same words there are for fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "# example_pairs is a list of lists where each list contains 2 * window_size elements. \n",
    "# each element will be a pair of (example, label)\n",
    "# map data into concrete X/Y input output lists\n",
    "\n",
    "features, labels = [], []\n",
    "#features = [elm[0] for elm in li for lin in example_pairs]\n",
    "#labels = [elm[1] for elm in li for li in example_pairs]\n",
    "for li in example_pairs:\n",
    "    features = features + [elm[0] for elm in li]\n",
    "    labels = labels + [elm[1] for elm in li]\n",
    "\n",
    "# TODO - implement model - probably will be written up in a module and imported here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
